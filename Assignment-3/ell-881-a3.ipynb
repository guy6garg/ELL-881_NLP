{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-28T05:16:41.914904Z","iopub.execute_input":"2023-04-28T05:16:41.915869Z","iopub.status.idle":"2023-04-28T05:16:44.061965Z","shell.execute_reply.started":"2023-04-28T05:16:41.915829Z","shell.execute_reply":"2023-04-28T05:16:44.060884Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install seqeval\n!pip install transformers\nfrom seqeval.metrics import f1_score, accuracy_score, precision_score, recall_score","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:16:44.066118Z","iopub.execute_input":"2023-04-28T05:16:44.066568Z","iopub.status.idle":"2023-04-28T05:17:11.396178Z","shell.execute_reply.started":"2023-04-28T05:16:44.066538Z","shell.execute_reply":"2023-04-28T05:17:11.395079Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval) (1.21.6)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval) (1.0.2)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16179 sha256=153b8592aa6be08339304dde11aca33d612ce297a5949d8f267755b8cef04a17\n  Stored in directory: /root/.cache/pip/wheels/b2/a1/b7/0d3b008d0c77cd57332d724b92cf7650b4185b493dc785f00a\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.27.4)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertModel\nbert = BertModel.from_pretrained('bert-base-cased')","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:17:11.398265Z","iopub.execute_input":"2023-04-28T05:17:11.398654Z","iopub.status.idle":"2023-04-28T05:17:19.076936Z","shell.execute_reply.started":"2023-04-28T05:17:11.398614Z","shell.execute_reply":"2023-04-28T05:17:19.075863Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf2824040f8549d5ba48cdb383c409b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"102dab4416094cb282bdf2a891e99ed4"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertTokenizerFast,BertForTokenClassification\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\ntokenizer.add_tokens(['B_geo','I_geo','B_per','I_per','B_org','I_org','B_gpe','I_gpe','B_tim','I_tim','B_art','I_art','B_eve','I_eve','B_nat','I_nat'])\ntokenizer","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:17:19.080244Z","iopub.execute_input":"2023-04-28T05:17:19.080948Z","iopub.status.idle":"2023-04-28T05:17:22.131807Z","shell.execute_reply.started":"2023-04-28T05:17:19.080901Z","shell.execute_reply":"2023-04-28T05:17:22.130644Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f07d4106ddf24e6da092f6d314c0b10b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"220eaffde4934691a40570279ddff562"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4ec4453dee5480b8a956a4c2450ed47"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"},"metadata":{}}]},{"cell_type":"code","source":"data = pd.read_csv('../input/ner-data/ner.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:17:22.133246Z","iopub.execute_input":"2023-04-28T05:17:22.133723Z","iopub.status.idle":"2023-04-28T05:17:22.370643Z","shell.execute_reply.started":"2023-04-28T05:17:22.133684Z","shell.execute_reply":"2023-04-28T05:17:22.369589Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:17:22.372115Z","iopub.execute_input":"2023-04-28T05:17:22.372511Z","iopub.status.idle":"2023-04-28T05:17:22.389843Z","shell.execute_reply.started":"2023-04-28T05:17:22.372473Z","shell.execute_reply":"2023-04-28T05:17:22.388933Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                text  \\\n0  Thousands of demonstrators have marched throug...   \n1  Iranian officials say they expect to get acces...   \n2  Helicopter gunships Saturday pounded militant ...   \n3  They left after a tense hour-long standoff wit...   \n4  U.N. relief coordinator Jan Egeland said Sunda...   \n\n                                              labels  \n0  O O O O O O B-geo O O O O O B-geo O O O O O B-...  \n1  B-gpe O O O O O O O O O O O O O O B-tim O O O ...  \n2  O O B-tim O O O O O B-geo O O O O O B-org O O ...  \n3                              O O O O O O O O O O O  \n4  B-geo O O B-per I-per O B-tim O B-geo O B-gpe ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Thousands of demonstrators have marched throug...</td>\n      <td>O O O O O O B-geo O O O O O B-geo O O O O O B-...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Iranian officials say they expect to get acces...</td>\n      <td>B-gpe O O O O O O O O O O O O O O B-tim O O O ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Helicopter gunships Saturday pounded militant ...</td>\n      <td>O O B-tim O O O O O B-geo O O O O O B-org O O ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>They left after a tense hour-long standoff wit...</td>\n      <td>O O O O O O O O O O O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>U.N. relief coordinator Jan Egeland said Sunda...</td>\n      <td>B-geo O O B-per I-per O B-tim O B-geo O B-gpe ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Warning** \n\n\nIn this Notebook, we will perform NER for Person,Location and Organization. So I will replace other tags with 'O' in the following funtions. If you intend to work for all the entities in the dataset, just remove the following \"preprocess_dataset\" function and call to it in the following line. Then again, you need to change 'label_to_ids' and 'ids_to_label' and the number of output from BERT model correspondingly. ","metadata":{}},{"cell_type":"markdown","source":"### data=preprocess_dataset(data)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T19:38:13.272293Z","iopub.execute_input":"2023-04-22T19:38:13.272560Z","iopub.status.idle":"2023-04-22T19:38:13.644640Z","shell.execute_reply.started":"2023-04-22T19:38:13.272530Z","shell.execute_reply":"2023-04-22T19:38:13.643445Z"}}},{"cell_type":"code","source":"unique_tags = data.labels.apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0)\nunique_tags","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:17:22.391207Z","iopub.execute_input":"2023-04-28T05:17:22.391688Z","iopub.status.idle":"2023-04-28T05:17:43.330462Z","shell.execute_reply.started":"2023-04-28T05:17:22.391644Z","shell.execute_reply":"2023-04-28T05:17:43.329331Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"O        887908.0\nB-geo     37644.0\nB-gpe     15870.0\nB-tim     20333.0\nB-org     20143.0\nI-geo      7414.0\nB-per     16990.0\nI-per     17251.0\nI-org     16784.0\nI-tim      6528.0\nB-art       402.0\nI-art       297.0\nB-nat       201.0\nI-gpe       198.0\nI-nat        51.0\nB-eve       308.0\nI-eve       253.0\ndtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"labels = [i.split() for i in data['labels'].values.tolist()]\nunique_labels = set()\n\nfor lb in labels:\n        [unique_labels.add(i) for i in lb if i not in unique_labels]\nlabel_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\nids_to_label = {v: k for v, k in enumerate(sorted(unique_labels))}","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:17:43.332080Z","iopub.execute_input":"2023-04-28T05:17:43.332731Z","iopub.status.idle":"2023-04-28T05:17:43.472538Z","shell.execute_reply.started":"2023-04-28T05:17:43.332691Z","shell.execute_reply":"2023-04-28T05:17:43.471373Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(label_to_ids)\nprint(ids_to_label)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:17:43.473947Z","iopub.execute_input":"2023-04-28T05:17:43.474383Z","iopub.status.idle":"2023-04-28T05:17:43.480277Z","shell.execute_reply.started":"2023-04-28T05:17:43.474338Z","shell.execute_reply":"2023-04-28T05:17:43.479089Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"{'B-art': 0, 'B-eve': 1, 'B-geo': 2, 'B-gpe': 3, 'B-nat': 4, 'B-org': 5, 'B-per': 6, 'B-tim': 7, 'I-art': 8, 'I-eve': 9, 'I-geo': 10, 'I-gpe': 11, 'I-nat': 12, 'I-org': 13, 'I-per': 14, 'I-tim': 15, 'O': 16}\n{0: 'B-art', 1: 'B-eve', 2: 'B-geo', 3: 'B-gpe', 4: 'B-nat', 5: 'B-org', 6: 'B-per', 7: 'B-tim', 8: 'I-art', 9: 'I-eve', 10: 'I-geo', 11: 'I-gpe', 12: 'I-nat', 13: 'I-org', 14: 'I-per', 15: 'I-tim', 16: 'O'}\n","output_type":"stream"}]},{"cell_type":"code","source":"label_all_tokens = False\n\ndef adjust_label(texts, labels):\n    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n\n    word_ids = tokenized_inputs.word_ids()\n\n    previous_word_idx = None\n    label_ids = []\n\n    for word_idx in word_ids:\n\n        if word_idx is None:\n            label_ids.append(-100)\n\n        elif word_idx != previous_word_idx:\n            try:\n                label_ids.append(label_to_ids[labels[word_idx]])\n            except:\n                label_ids.append(-100)\n        else:\n            try:\n                label_ids.append(label_to_ids[labels[word_idx]] if label_all_tokens else -100)\n            except:\n                label_ids.append(-100)\n        previous_word_idx = word_idx\n\n    return label_ids\n\nclass Ner_Data(torch.utils.data.Dataset):\n\n    def __init__(self, df):\n\n        lb = [i.split() for i in df['labels'].values.tolist()]\n        txt = df['text'].values.tolist()\n        self.texts = [tokenizer(str(i), padding='max_length', max_length = 512, truncation=True) for i in txt]\n        self.labels = [adjust_label(i,j) for i,j in zip(txt, lb)]\n\n    def __len__(self):\n\n        return len(self.labels)\n\n    def get_batch_data(self, idx):\n\n        return self.texts[idx]\n\n    def get_batch_labels(self, idx):\n\n        return torch.LongTensor(self.labels[idx])\n\n    def __getitem__(self, idx):\n\n        batch_data = self.get_batch_data(idx)\n        batch_labels = self.get_batch_labels(idx)\n        item = {key: torch.as_tensor(val) for key, val in batch_data.items()}\n        item['labels'] = batch_labels\n        return item\n","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:17:43.489450Z","iopub.execute_input":"2023-04-28T05:17:43.490799Z","iopub.status.idle":"2023-04-28T05:17:43.504434Z","shell.execute_reply.started":"2023-04-28T05:17:43.490754Z","shell.execute_reply":"2023-04-28T05:17:43.503488Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# data = data[:1000]\ndf_train, df_val, df_test = np.split(data.sample(frac=1, random_state=42),\n                            [int(.8 * len(data)), int(.9 * len(data))])","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:17:43.505967Z","iopub.execute_input":"2023-04-28T05:17:43.506716Z","iopub.status.idle":"2023-04-28T05:17:43.528567Z","shell.execute_reply.started":"2023-04-28T05:17:43.506661Z","shell.execute_reply":"2023-04-28T05:17:43.527373Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.reset_index(drop = True)\ntrain_data = Ner_Data(df_train)\ndf_val = df_val.reset_index(drop = True)\nval_data = Ner_Data(df_val)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:17:43.532454Z","iopub.execute_input":"2023-04-28T05:17:43.532791Z","iopub.status.idle":"2023-04-28T05:18:08.025239Z","shell.execute_reply.started":"2023-04-28T05:17:43.532759Z","shell.execute_reply":"2023-04-28T05:18:08.024176Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(df_train.iloc[10]['text'])\nprint(df_train.iloc[10]['labels'])\nprint(train_data[10]['labels'])\nprint((train_data[10]['input_ids']))","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:18:08.027076Z","iopub.execute_input":"2023-04-28T05:18:08.027495Z","iopub.status.idle":"2023-04-28T05:18:08.044241Z","shell.execute_reply.started":"2023-04-28T05:18:08.027452Z","shell.execute_reply":"2023-04-28T05:18:08.042964Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Officials estimate that some 5,00,000 people living in Latin American countries such as Cuba , Argentina , Mexico , Venezuela , Chile and Uruguay are eligible for citizenship .\nO O O O O O O O O B-gpe O O O B-geo O B-geo O B-geo O B-geo O B-geo O B-geo O O O O O\ntensor([-100,   16, -100,   16,   16,   16,   16,   16,   16,   16,   16,    3,\n          16,   16,   16,    2,   16,    2,   16,    2,   16,    2,   16,    2,\n          16,    2,   16,   16,   16,   16,   16, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100])\ntensor([  101,  9018,  1116, 10301,  1115,  1199,   126,   117,  3135,   117,\n         1288,  1234,  1690,  1107,  2911,  1237,  2182,  1216,  1112,  6881,\n          117,  4904,   117,  2470,   117,  7917,   117,  6504,  1105, 11752,\n         1132,  7408,  1111,  9709,   119,   102,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0])\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data.__getitem__(10)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:18:08.045695Z","iopub.execute_input":"2023-04-28T05:18:08.046244Z","iopub.status.idle":"2023-04-28T05:18:08.063722Z","shell.execute_reply.started":"2023-04-28T05:18:08.046206Z","shell.execute_reply":"2023-04-28T05:18:08.062573Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([  101,  9018,  1116, 10301,  1115,  1199,   126,   117,  3135,   117,\n          1288,  1234,  1690,  1107,  2911,  1237,  2182,  1216,  1112,  6881,\n           117,  4904,   117,  2470,   117,  7917,   117,  6504,  1105, 11752,\n          1132,  7408,  1111,  9709,   119,   102,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0]),\n 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]),\n 'labels': tensor([-100,   16, -100,   16,   16,   16,   16,   16,   16,   16,   16,    3,\n           16,   16,   16,    2,   16,    2,   16,    2,   16,    2,   16,    2,\n           16,    2,   16,   16,   16,   16,   16, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100])}"},"metadata":{}}]},{"cell_type":"code","source":"print(train_data[100])\nprint(train_data[100]['input_ids'].detach().numpy())\nprint(tokenizer.convert_ids_to_tokens(train_data[100]['input_ids'].detach().numpy()))\nprint('#####')\nfor i in train_data[100]['labels'].detach().numpy():\n    print(i)\n    print(ids_to_label.get(i))","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:18:08.065289Z","iopub.execute_input":"2023-04-28T05:18:08.065718Z","iopub.status.idle":"2023-04-28T05:18:08.090801Z","shell.execute_reply.started":"2023-04-28T05:18:08.065682Z","shell.execute_reply":"2023-04-28T05:18:08.089690Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([  101,  1109,   158,   119,   156,   119,  7988,  1111, 11023,  1314,\n         1989,  2103,  1861,  3690,   119,   102,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([-100,   16,    5,   13,   13,   13,   16,   16,   16,   16,   16,   16,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100])}\n[  101  1109   158   119   156   119  7988  1111 11023  1314  1989  2103\n  1861  3690   119   102     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0]\n['[CLS]', 'The', 'U', '.', 'S', '.', 'Campaign', 'for', 'Burma', 'last', 'week', 'reported', 'similar', 'attacks', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n#####\n-100\nNone\n16\nO\n5\nB-org\n13\nI-org\n13\nI-org\n13\nI-org\n16\nO\n16\nO\n16\nO\n16\nO\n16\nO\n16\nO\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_data, batch_size=16,shuffle=False)\nval_dataloader = DataLoader(val_data, batch_size=16, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:18:08.092448Z","iopub.execute_input":"2023-04-28T05:18:08.092797Z","iopub.status.idle":"2023-04-28T05:18:08.098628Z","shell.execute_reply.started":"2023-04-28T05:18:08.092760Z","shell.execute_reply":"2023-04-28T05:18:08.097333Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.0001\nbatch_size = 16\nepochs = 6","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:18:08.100099Z","iopub.execute_input":"2023-04-28T05:18:08.101167Z","iopub.status.idle":"2023-04-28T05:18:08.108421Z","shell.execute_reply.started":"2023-04-28T05:18:08.101131Z","shell.execute_reply":"2023-04-28T05:18:08.107270Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def train_loop(train_dataloader,val_dataloader,model, optimizer,epochs):\n    size = len(train_dataloader.dataset)\n    Val_loss, Train_loss = [], []\n    Train_with_accuracy, Val_with_accuracy = [], []\n    Train_without_accuracy, Val_without_accuracy = [], []\n    for epoch_num in range(epochs):\n        print(f'Epochs: {epoch_num + 1}\\n-------------------------------')\n        \n        train_loss, train_with_accuracy, train_without_accuracy = 0, 0, 0\n        train_steps = 0\n        model.train()\n        for idx,batch in enumerate(train_dataloader):\n            optimizer.zero_grad()\n            ids= batch['input_ids'].to(device)\n            mask= batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            preds = model(input_ids=ids, attention_mask=mask ,labels = labels)\n    #         print(f\"loss: {loss.item()}\")\n            loss = preds['loss']\n            logits = preds['logits']\n            train_loss+=loss.item()\n            train_steps+=1\n            # computing train accuracy\n            flattened_targets = labels.view(-1) \n            active_logits = logits.view(-1, model.num_labels) \n            flattened_predictions = torch.argmax(active_logits, axis=1) \n\n            # computing accuracy at active labels\n            labels_without = labels\n            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n\n            labels = torch.masked_select(flattened_targets, active_accuracy)\n            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n\n            tmp_train_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n            train_with_accuracy += tmp_train_accuracy\n            # computing accuracy at active labels excluding O\n            active_without_accuracy = []\n            for label in labels_without.view(-1):\n                if(label == label_to_ids['O'] or label == -100):\n                    active_without_accuracy.append(False)\n                else:\n                    active_without_accuracy.append(True)\n\n            active_without_accuracy = torch.as_tensor(active_without_accuracy)\n            active_without_accuracy = active_without_accuracy.to(device)\n\n            labels_without = torch.masked_select(flattened_targets, active_without_accuracy)\n            predictions_without = torch.masked_select(flattened_predictions, active_without_accuracy)            \n            tmp_train_without_accuracy = accuracy_score(labels_without.cpu().numpy(), predictions_without.cpu().numpy())\n            train_without_accuracy += tmp_train_without_accuracy\n\n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            if((idx+1) % 1000==0):\n                print(f\"Train Loss per 1000 steps: {train_loss/(idx+1):>4f} [{(idx+1):>5d}/{size/batch_size}]\")\n            \n        model.eval()\n        eval_loss, eval_with_accuracy, eval_without_accuracy = 0, 0, 0\n        eval_steps = 0\n\n        with torch.no_grad():\n            for idx, batch in enumerate(val_dataloader):\n                ids = batch['input_ids'].to(device, dtype = torch.long)\n                mask = batch['attention_mask'].to(device, dtype = torch.long)\n                labels = batch['labels'].to(device, dtype = torch.long)\n                preds = model(input_ids=ids, attention_mask=mask, labels=labels)\n\n                loss = preds['loss']\n                eval_logits = preds['logits'] \n    #             print(loss.item())\n                eval_loss += loss.item()\n                eval_steps += 1\n\n                if (idx+1) % 100==0:\n                    loss_step = eval_loss/eval_steps\n                    print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n\n                # computing evaluation accuracy\n                flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n                active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n                flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n\n                # only compute accuracy at active labels\n                labels_without = labels\n                active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n\n                labels = torch.masked_select(flattened_targets, active_accuracy)\n                predictions = torch.masked_select(flattened_predictions, active_accuracy)\n\n                tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n                eval_with_accuracy += tmp_eval_accuracy\n       # only compute accuracy at active labels excluding O\n                active_without_accuracy = []\n                for label in labels_without.view(-1):\n                    if(label == label_to_ids['O'] or label == -100):\n                        active_without_accuracy.append(False)\n                    else:\n                        active_without_accuracy.append(True)\n\n                active_without_accuracy = torch.as_tensor(active_without_accuracy)\n                active_without_accuracy = active_without_accuracy.to(device)\n    #             print(active_without_accuracy.size())\n    #             print(flattened_targets.size())\n\n                labels_without = torch.masked_select(flattened_targets, active_without_accuracy)\n                predictions_without = torch.masked_select(flattened_predictions, active_without_accuracy)            \n                tmp_eval_without_accuracy = accuracy_score(labels_without.cpu().numpy(), predictions_without.cpu().numpy())\n                eval_without_accuracy += tmp_eval_without_accuracy\n\n        train_loss = train_loss / train_steps\n        train_with_accuracy = train_with_accuracy / train_steps\n        train_without_accuracy = train_without_accuracy / train_steps\n        Train_loss.append(train_loss)\n        Train_with_accuracy.append(train_with_accuracy)\n        Train_without_accuracy.append(train_without_accuracy)\n        print(f\"Total Train Loss: {train_loss}\")\n        print(f\"Total Train Accuracy With O: {train_with_accuracy}\")\n        print(f\"Total Train Accuracy Without O: {train_without_accuracy}\")\n        \n        eval_loss = eval_loss / eval_steps\n        eval_with_accuracy = eval_with_accuracy / eval_steps\n        eval_without_accuracy = eval_without_accuracy / eval_steps\n        Val_loss.append(eval_loss)\n        Val_with_accuracy.append(eval_with_accuracy)\n        Val_without_accuracy.append(eval_without_accuracy)\n        print(f\"Total Validation Loss: {eval_loss}\")\n        print(f\"Total Validation Accuracy With O: {eval_with_accuracy}\")\n        print(f\"Total Validation Accuracy Without O: {eval_without_accuracy}\")\n    \n    item = {}\n    item['train_loss'] = Train_loss\n    item['val_loss'] = Val_loss\n    item['train_with_acc'] = Train_with_accuracy\n    item['val_with_acc'] = Val_with_accuracy\n    item['train_without_acc'] = Train_without_accuracy\n    item['val_without_acc'] = Val_without_accuracy\n    return item\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:18:08.111038Z","iopub.execute_input":"2023-04-28T05:18:08.111491Z","iopub.status.idle":"2023-04-28T05:18:08.135378Z","shell.execute_reply.started":"2023-04-28T05:18:08.111451Z","shell.execute_reply":"2023-04-28T05:18:08.134302Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(label_to_ids))\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:18:08.137118Z","iopub.execute_input":"2023-04-28T05:18:08.137552Z","iopub.status.idle":"2023-04-28T05:18:12.660071Z","shell.execute_reply.started":"2023-04-28T05:18:08.137516Z","shell.execute_reply":"2023-04-28T05:18:12.659008Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=17, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nresults = train_loop(train_dataloader,val_dataloader, model, optimizer,epochs)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:18:12.661601Z","iopub.execute_input":"2023-04-28T05:18:12.662515Z","iopub.status.idle":"2023-04-28T10:22:07.500239Z","shell.execute_reply.started":"2023-04-28T05:18:12.662472Z","shell.execute_reply":"2023-04-28T10:22:07.499076Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epochs: 1\n-------------------------------\nTrain Loss per 1000 steps: 0.246665 [ 1000/2397.9375]\nTrain Loss per 1000 steps: 0.205349 [ 2000/2397.9375]\nValidation loss per 100 evaluation steps: 0.14794150680303575\nValidation loss per 100 evaluation steps: 0.14712351068854332\nValidation loss per 100 evaluation steps: 0.14879861485213042\nTotal Train Loss: 0.196488976012527\nTotal Train Accuracy With O: 0.9399010747627506\nTotal Train Accuracy Without O: 0.6793964346812459\nTotal Validation Loss: 0.14879861485213042\nTotal Validation Accuracy With O: 0.9522722118207501\nTotal Validation Accuracy Without O: 0.7536994730591401\nEpochs: 2\n-------------------------------\nTrain Loss per 1000 steps: 0.133655 [ 1000/2397.9375]\nTrain Loss per 1000 steps: 0.129069 [ 2000/2397.9375]\nValidation loss per 100 evaluation steps: 0.13477992333471775\nValidation loss per 100 evaluation steps: 0.13564114954322576\nValidation loss per 100 evaluation steps: 0.1371049087991317\nTotal Train Loss: 0.12779463541843897\nTotal Train Accuracy With O: 0.958917228819887\nTotal Train Accuracy Without O: 0.7841444249933075\nTotal Validation Loss: 0.1371049087991317\nTotal Validation Accuracy With O: 0.9574312880289911\nTotal Validation Accuracy Without O: 0.7700392273646675\nEpochs: 3\n-------------------------------\nTrain Loss per 1000 steps: 0.111230 [ 1000/2397.9375]\nTrain Loss per 1000 steps: 0.105534 [ 2000/2397.9375]\nValidation loss per 100 evaluation steps: 0.1343186932057142\nValidation loss per 100 evaluation steps: 0.13529235993977637\nValidation loss per 100 evaluation steps: 0.13551968909489612\nTotal Train Loss: 0.10452239948353599\nTotal Train Accuracy With O: 0.9655422923817224\nTotal Train Accuracy Without O: 0.8200089197957443\nTotal Validation Loss: 0.13551968909489612\nTotal Validation Accuracy With O: 0.9600185850553906\nTotal Validation Accuracy Without O: 0.7788355701003432\nEpochs: 4\n-------------------------------\nTrain Loss per 1000 steps: 0.093398 [ 1000/2397.9375]\nTrain Loss per 1000 steps: 0.091588 [ 2000/2397.9375]\nValidation loss per 100 evaluation steps: 0.1412457975745201\nValidation loss per 100 evaluation steps: 0.14211649749428035\nValidation loss per 100 evaluation steps: 0.14105190861970185\nTotal Train Loss: 0.09059331273688026\nTotal Train Accuracy With O: 0.969866783441361\nTotal Train Accuracy Without O: 0.8441455873986892\nTotal Validation Loss: 0.14105190861970185\nTotal Validation Accuracy With O: 0.9588027270324776\nTotal Validation Accuracy Without O: 0.7752916602285052\nEpochs: 5\n-------------------------------\nTrain Loss per 1000 steps: 0.086703 [ 1000/2397.9375]\nTrain Loss per 1000 steps: 0.083391 [ 2000/2397.9375]\nValidation loss per 100 evaluation steps: 0.14253742039203643\nValidation loss per 100 evaluation steps: 0.14136963131837546\nValidation loss per 100 evaluation steps: 0.1393964004982263\nTotal Train Loss: 0.08192089112596687\nTotal Train Accuracy With O: 0.9728442163506116\nTotal Train Accuracy Without O: 0.8615830046159287\nTotal Validation Loss: 0.1393964004982263\nTotal Validation Accuracy With O: 0.9632010826491342\nTotal Validation Accuracy Without O: 0.8059281741557703\nEpochs: 6\n-------------------------------\nTrain Loss per 1000 steps: 0.071500 [ 1000/2397.9375]\nTrain Loss per 1000 steps: 0.068578 [ 2000/2397.9375]\nValidation loss per 100 evaluation steps: 0.14351764310151338\nValidation loss per 100 evaluation steps: 0.14152177307754754\nValidation loss per 100 evaluation steps: 0.1405267407745123\nTotal Train Loss: 0.06845852069760998\nTotal Train Accuracy With O: 0.9769524586234792\nTotal Train Accuracy Without O: 0.883641022339565\nTotal Validation Loss: 0.1405267407745123\nTotal Validation Accuracy With O: 0.9637565649175535\nTotal Validation Accuracy Without O: 0.805436881486579\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nEPOCH = np.arange(1,7)\nplt.plot(EPOCH,results['train_loss'], label = 'train_loss')\n\nplt.plot(EPOCH,results['val_loss'], label = 'val_loss')\nplt.xlabel(\"EPOCHS\")\nplt.ylabel(\"Loss\")\nplt.title('Training v/s Validation Loss for Experiment - II')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-28T10:26:18.878110Z","iopub.execute_input":"2023-04-28T10:26:18.878507Z","iopub.status.idle":"2023-04-28T10:26:19.098290Z","shell.execute_reply.started":"2023-04-28T10:26:18.878471Z","shell.execute_reply":"2023-04-28T10:26:19.097260Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"test_dataset = df_test.reset_index(drop = True)\ntest_data = Ner_Data(test_dataset)\ntest_data_loader = DataLoader(test_data, batch_size=4, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T10:22:07.747540Z","iopub.execute_input":"2023-04-28T10:22:07.747838Z","iopub.status.idle":"2023-04-28T10:22:10.320070Z","shell.execute_reply.started":"2023-04-28T10:22:07.747810Z","shell.execute_reply":"2023-04-28T10:22:10.319044Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def test(model, testing_loader):\n    test_loss, test_with_accuracy, test_without_accuracy = 0, 0, 0\n    test_steps = 0\n    test_preds, test_labels = [], []\n    \n    for idx, batch in enumerate(testing_loader):\n\n        ids = batch['input_ids'].to(device, dtype = torch.long)\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\n        labels = batch['labels'].to(device, dtype = torch.long)\n        preds= model(input_ids=ids, attention_mask=mask, labels=labels)\n\n        loss = preds['loss']\n        logits = preds['logits'] \n\n        test_loss += loss.item()\n        test_steps += 1\n        # compute evaluation accuracy\n        flattened_targets = labels.view(-1) \n        active_logits = logits.view(-1, model.num_labels) \n        flattened_predictions = torch.argmax(active_logits, axis=1)\n\n        # only compute accuracy at active labels\n        labels_without = labels\n        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n\n        labels = torch.masked_select(flattened_targets, active_accuracy)\n        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n\n        test_labels.extend(labels)\n        test_preds.extend(predictions)\n\n        tmp_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n        test_with_accuracy += tmp_accuracy\n\n        active_without_accuracy = []\n        for label in labels_without.view(-1):\n            if(label == label_to_ids['O'] or label == -100):\n                active_without_accuracy.append(False)\n            else:\n                active_without_accuracy.append(True)\n\n        active_without_accuracy = torch.as_tensor(active_without_accuracy)\n        active_without_accuracy = active_without_accuracy.to(device)\n#             print(active_without_accuracy.size())\n#             print(flattened_targets.size())\n\n        labels_without = torch.masked_select(flattened_targets, active_without_accuracy)\n        predictions_without = torch.masked_select(flattened_predictions, active_without_accuracy)            \n        tmp_without_accuracy = accuracy_score(labels_without.cpu().numpy(), predictions_without.cpu().numpy())\n        test_without_accuracy += tmp_without_accuracy\n            \n    labels = [ids_to_label[id.item()] for id in test_labels]\n    predictions = [ids_to_label[id.item()] for id in test_preds]\n    test_loss = test_loss / test_steps\n    test_with_accuracy = test_with_accuracy / test_steps\n    test_without_accuracy = test_without_accuracy / test_steps\n    print(f\"Test Loss: {test_loss}\")\n    print(f\"Test Accuracy: {test_with_accuracy}\")\n    print(f\"Test Accuracy Without O: {test_without_accuracy}\")\n    return labels, predictions","metadata":{"execution":{"iopub.status.busy":"2023-04-28T10:22:10.321584Z","iopub.execute_input":"2023-04-28T10:22:10.321935Z","iopub.status.idle":"2023-04-28T10:22:10.337498Z","shell.execute_reply.started":"2023-04-28T10:22:10.321896Z","shell.execute_reply":"2023-04-28T10:22:10.336427Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"labels, predictions = test(model, test_data_loader)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T10:22:10.339422Z","iopub.execute_input":"2023-04-28T10:22:10.339841Z","iopub.status.idle":"2023-04-28T10:25:28.455668Z","shell.execute_reply.started":"2023-04-28T10:22:10.339796Z","shell.execute_reply":"2023-04-28T10:25:28.453390Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Test Loss: 0.14131849652683065\nTest Accuracy: 0.9641340770597987\nTest Accuracy Without O: 0.8061552460543501\n","output_type":"stream"}]},{"cell_type":"code","source":"from seqeval.metrics import classification_report\nprint(classification_report([labels], [predictions]))","metadata":{"execution":{"iopub.status.busy":"2023-04-28T10:25:28.457451Z","iopub.execute_input":"2023-04-28T10:25:28.457803Z","iopub.status.idle":"2023-04-28T10:25:30.193283Z","shell.execute_reply.started":"2023-04-28T10:25:28.457764Z","shell.execute_reply":"2023-04-28T10:25:30.191975Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         art       0.18      0.10      0.13        41\n         eve       0.22      0.13      0.17        15\n         geo       0.79      0.86      0.83      3788\n         gpe       0.97      0.89      0.93      1636\n         nat       1.00      0.14      0.25        28\n         org       0.71      0.58      0.64      2003\n         per       0.72      0.74      0.73      1676\n         tim       0.86      0.83      0.84      2031\n\n   micro avg       0.80      0.79      0.80     11218\n   macro avg       0.68      0.54      0.56     11218\nweighted avg       0.80      0.79      0.79     11218\n\n","output_type":"stream"}]},{"cell_type":"code","source":"def align_word_ids(texts):\n  \n    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n    word_ids = tokenized_inputs.word_ids()\n    \n    previous_word_idx = None\n    label_ids = []\n\n    for word_idx in word_ids:\n\n        if word_idx is None:\n            label_ids.append(-100)\n\n        elif word_idx != previous_word_idx:\n            try:\n                label_ids.append(1)\n            except:\n                label_ids.append(-100)\n        else:\n            try:\n                label_ids.append(1 if label_all_tokens else -100)\n            except:\n                label_ids.append(-100)\n        previous_word_idx = word_idx\n\n    return label_ids\n\n\ndef evaluate_one_text(model, sentence):\n    text_input = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True)\n    text = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n    mask = text['attention_mask'].to(device)\n    input_id = text['input_ids'].to(device)\n    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n\n    logits = model(input_id, mask, None)\n    logits_clean = logits[0][label_ids != -100]\n\n    predictions = logits_clean.argmax(dim=1).tolist()\n    prediction_label = [ids_to_label[i] for i in predictions]\n    text_tokens = tokenizer.convert_ids_to_tokens(text_input['input_ids'])\n    \n    print(sentence)\n    print(\"Tokenized Sentence:\",text_tokens[1:text_tokens.index('[SEP]')])\n    print(\"Predicted Labels:\",prediction_label)\n            \nevaluate_one_text(model, 'Sundar Pichai is the CEO of Google .')\n","metadata":{"execution":{"iopub.status.busy":"2023-04-28T10:25:30.195150Z","iopub.execute_input":"2023-04-28T10:25:30.195619Z","iopub.status.idle":"2023-04-28T10:25:30.235201Z","shell.execute_reply.started":"2023-04-28T10:25:30.195579Z","shell.execute_reply":"2023-04-28T10:25:30.233910Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Sundar Pichai is the CEO of Google .\nTokenized Sentence: ['Sun', '##dar', 'Pi', '##cha', '##i', 'is', 'the', 'CEO', 'of', 'Google', '.']\nPredicted Labels: ['B-per', 'I-per', 'O', 'O', 'O', 'O', 'B-org', 'O']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## SAVING THE MODEL","metadata":{}},{"cell_type":"code","source":"torch.save(model, './model.pth')","metadata":{"execution":{"iopub.status.busy":"2023-04-28T10:25:30.236753Z","iopub.execute_input":"2023-04-28T10:25:30.237296Z","iopub.status.idle":"2023-04-28T10:25:30.856435Z","shell.execute_reply.started":"2023-04-28T10:25:30.237259Z","shell.execute_reply":"2023-04-28T10:25:30.855367Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# model3 = torch.load('./model.pth')","metadata":{"execution":{"iopub.status.busy":"2023-04-28T10:25:30.862011Z","iopub.execute_input":"2023-04-28T10:25:30.862836Z","iopub.status.idle":"2023-04-28T10:25:30.867614Z","shell.execute_reply.started":"2023-04-28T10:25:30.862795Z","shell.execute_reply":"2023-04-28T10:25:30.866351Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}