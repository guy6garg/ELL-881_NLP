{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-27T15:15:53.417846Z","iopub.execute_input":"2023-04-27T15:15:53.418210Z","iopub.status.idle":"2023-04-27T15:15:54.914852Z","shell.execute_reply.started":"2023-04-27T15:15:53.418129Z","shell.execute_reply":"2023-04-27T15:15:54.914073Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install seqeval\n!pip install transformers\nfrom seqeval.metrics import f1_score, accuracy_score, precision_score, recall_score","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:15:54.918260Z","iopub.execute_input":"2023-04-27T15:15:54.920046Z","iopub.status.idle":"2023-04-27T15:16:06.049890Z","shell.execute_reply.started":"2023-04-27T15:15:54.919922Z","shell.execute_reply":"2023-04-27T15:16:06.049103Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n     |████████████████████████████████| 43 kB 1.8 MB/s             \n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval) (1.19.5)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval) (0.23.2)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (3.0.0)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.7.2)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=16029ae04b190d682268da9d25c52e7ab21bb1e5dc757fcbd009d87591a55669\n  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertModel\nbert = BertModel.from_pretrained('bert-base-cased')","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:16:06.051235Z","iopub.execute_input":"2023-04-27T15:16:06.051487Z","iopub.status.idle":"2023-04-27T15:16:27.915499Z","shell.execute_reply.started":"2023-04-27T15:16:06.051447Z","shell.execute_reply":"2023-04-27T15:16:27.914687Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"150071ad519f4a45971e3708c9e1e580"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16c6c7930c8242a98ec64666d5b9e206"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertTokenizerFast,BertForTokenClassification\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\ntokenizer.add_tokens(['B_geo','I_geo','B_per','I_per','B_org','I_org','B_gpe','I_gpe','B_tim','I_tim','B_art','I_art','B_eve','I_eve','B_nat','I_nat'])\ntokenizer","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:16:27.917520Z","iopub.execute_input":"2023-04-27T15:16:27.918891Z","iopub.status.idle":"2023-04-27T15:16:30.770880Z","shell.execute_reply.started":"2023-04-27T15:16:27.918861Z","shell.execute_reply":"2023-04-27T15:16:30.770018Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28ed86c5a78540a194bf0b883bc7c65b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef9131ec77cb4464bce52eda5ff2a251"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07d90125f6d74e75b941cccb6a80bee3"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"PreTrainedTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"},"metadata":{}}]},{"cell_type":"code","source":"data = pd.read_csv('../input/ner-data/ner.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:16:30.772223Z","iopub.execute_input":"2023-04-27T15:16:30.772552Z","iopub.status.idle":"2023-04-27T15:16:30.987592Z","shell.execute_reply.started":"2023-04-27T15:16:30.772514Z","shell.execute_reply":"2023-04-27T15:16:30.986855Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:16:30.990926Z","iopub.execute_input":"2023-04-27T15:16:30.991138Z","iopub.status.idle":"2023-04-27T15:16:31.008154Z","shell.execute_reply.started":"2023-04-27T15:16:30.991107Z","shell.execute_reply":"2023-04-27T15:16:31.007489Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                text  \\\n0  Thousands of demonstrators have marched throug...   \n1  Iranian officials say they expect to get acces...   \n2  Helicopter gunships Saturday pounded militant ...   \n3  They left after a tense hour-long standoff wit...   \n4  U.N. relief coordinator Jan Egeland said Sunda...   \n\n                                              labels  \n0  O O O O O O B-geo O O O O O B-geo O O O O O B-...  \n1  B-gpe O O O O O O O O O O O O O O B-tim O O O ...  \n2  O O B-tim O O O O O B-geo O O O O O B-org O O ...  \n3                              O O O O O O O O O O O  \n4  B-geo O O B-per I-per O B-tim O B-geo O B-gpe ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Thousands of demonstrators have marched throug...</td>\n      <td>O O O O O O B-geo O O O O O B-geo O O O O O B-...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Iranian officials say they expect to get acces...</td>\n      <td>B-gpe O O O O O O O O O O O O O O B-tim O O O ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Helicopter gunships Saturday pounded militant ...</td>\n      <td>O O B-tim O O O O O B-geo O O O O O B-org O O ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>They left after a tense hour-long standoff wit...</td>\n      <td>O O O O O O O O O O O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>U.N. relief coordinator Jan Egeland said Sunda...</td>\n      <td>B-geo O O B-per I-per O B-tim O B-geo O B-gpe ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Warning** \n\n\nIn this Notebook, we will perform NER for Person,Location and Organization. So I will replace other tags with 'O' in the following funtions. If you intend to work for all the entities in the dataset, just remove the following \"preprocess_dataset\" function and call to it in the following line. Then again, you need to change 'label_to_ids' and 'ids_to_label' and the number of output from BERT model correspondingly. ","metadata":{}},{"cell_type":"markdown","source":"### data=preprocess_dataset(data)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T19:38:13.272293Z","iopub.execute_input":"2023-04-22T19:38:13.272560Z","iopub.status.idle":"2023-04-22T19:38:13.644640Z","shell.execute_reply.started":"2023-04-22T19:38:13.272530Z","shell.execute_reply":"2023-04-22T19:38:13.643445Z"}}},{"cell_type":"code","source":"unique_tags = data.labels.apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0)\nunique_tags","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:16:31.015971Z","iopub.execute_input":"2023-04-27T15:16:31.016268Z","iopub.status.idle":"2023-04-27T15:16:52.406548Z","shell.execute_reply.started":"2023-04-27T15:16:31.016235Z","shell.execute_reply":"2023-04-27T15:16:52.405722Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"O        887908.0\nB-geo     37644.0\nB-gpe     15870.0\nB-tim     20333.0\nB-org     20143.0\nI-geo      7414.0\nB-per     16990.0\nI-per     17251.0\nI-org     16784.0\nI-tim      6528.0\nB-art       402.0\nI-art       297.0\nB-nat       201.0\nI-gpe       198.0\nI-nat        51.0\nB-eve       308.0\nI-eve       253.0\ndtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"labels = [i.split() for i in data['labels'].values.tolist()]\nunique_labels = set()\n\nfor lb in labels:\n        [unique_labels.add(i) for i in lb if i not in unique_labels]\nlabel_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\nids_to_label = {v: k for v, k in enumerate(sorted(unique_labels))}","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:16:52.407723Z","iopub.execute_input":"2023-04-27T15:16:52.408011Z","iopub.status.idle":"2023-04-27T15:16:52.531114Z","shell.execute_reply.started":"2023-04-27T15:16:52.407972Z","shell.execute_reply":"2023-04-27T15:16:52.530341Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(label_to_ids)\nprint(ids_to_label)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:16:52.534426Z","iopub.execute_input":"2023-04-27T15:16:52.534710Z","iopub.status.idle":"2023-04-27T15:16:52.541203Z","shell.execute_reply.started":"2023-04-27T15:16:52.534671Z","shell.execute_reply":"2023-04-27T15:16:52.540480Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"{'B-art': 0, 'B-eve': 1, 'B-geo': 2, 'B-gpe': 3, 'B-nat': 4, 'B-org': 5, 'B-per': 6, 'B-tim': 7, 'I-art': 8, 'I-eve': 9, 'I-geo': 10, 'I-gpe': 11, 'I-nat': 12, 'I-org': 13, 'I-per': 14, 'I-tim': 15, 'O': 16}\n{0: 'B-art', 1: 'B-eve', 2: 'B-geo', 3: 'B-gpe', 4: 'B-nat', 5: 'B-org', 6: 'B-per', 7: 'B-tim', 8: 'I-art', 9: 'I-eve', 10: 'I-geo', 11: 'I-gpe', 12: 'I-nat', 13: 'I-org', 14: 'I-per', 15: 'I-tim', 16: 'O'}\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n    \"\"\"\n    Word piece tokenization makes it difficult to match word labels\n    back up with individual word pieces. This function tokenizes each\n    word one at a time so that it is easier to preserve the correct\n    label for each subword. It is, of course, a bit slower in processing\n    time, but it will help our model achieve higher accuracy.\n    \"\"\"\n    tokenized_sentence = []\n    labels = []\n    \n    for word, label in zip(sentence, text_labels):\n        tokenized_word = tokenizer.tokenize(word)\n        n_subwords = len(tokenized_word)\n        tokenized_sentence.extend(tokenized_word)\n        labels.extend([label] * n_subwords)\n    return tokenized_sentence, labels","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:16:52.542675Z","iopub.execute_input":"2023-04-27T15:16:52.543535Z","iopub.status.idle":"2023-04-27T15:16:52.550873Z","shell.execute_reply.started":"2023-04-27T15:16:52.543499Z","shell.execute_reply":"2023-04-27T15:16:52.550189Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"label_all_tokens = True\n\ndef adjust_label(texts, labels):\n    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n\n    word_ids = tokenized_inputs.word_ids()\n\n    previous_word_idx = None\n    label_ids = []\n\n    for word_idx in word_ids:\n\n        if word_idx is None:\n            label_ids.append(-100)\n\n        elif word_idx != previous_word_idx:\n            try:\n                label_ids.append(label_to_ids[labels[word_idx]])\n            except:\n                label_ids.append(-100)\n        else:\n            try:\n                label_ids.append(label_to_ids[labels[word_idx]] if label_all_tokens else -100)\n            except:\n                label_ids.append(-100)\n        previous_word_idx = word_idx\n\n    return label_ids\n\nclass Ner_Data(torch.utils.data.Dataset):\n\n    def __init__(self, df):\n\n        lb = [i.split() for i in df['labels'].values.tolist()]\n        txt = df['text'].values.tolist()\n        self.texts = [tokenizer(str(i), padding='max_length', max_length = 512, truncation=True) for i in txt]\n        self.labels = [adjust_label(i,j) for i,j in zip(txt, lb)]\n\n    def __len__(self):\n\n        return len(self.labels)\n\n    def get_batch_data(self, idx):\n\n        return self.texts[idx]\n\n    def get_batch_labels(self, idx):\n\n        return torch.LongTensor(self.labels[idx])\n\n    def __getitem__(self, idx):\n\n        batch_data = self.get_batch_data(idx)\n        batch_labels = self.get_batch_labels(idx)\n        item = {key: torch.as_tensor(val) for key, val in batch_data.items()}\n        item['labels'] = batch_labels\n        return item\n","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:16:52.552078Z","iopub.execute_input":"2023-04-27T15:16:52.552511Z","iopub.status.idle":"2023-04-27T15:16:52.566076Z","shell.execute_reply.started":"2023-04-27T15:16:52.552477Z","shell.execute_reply":"2023-04-27T15:16:52.565271Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# data = data[:1000]\ndf_train, df_val, df_test = np.split(data.sample(frac=1, random_state=42),\n                            [int(.8 * len(data)), int(.9 * len(data))])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:16:52.567368Z","iopub.execute_input":"2023-04-27T15:16:52.567631Z","iopub.status.idle":"2023-04-27T15:16:52.587765Z","shell.execute_reply.started":"2023-04-27T15:16:52.567594Z","shell.execute_reply":"2023-04-27T15:16:52.587133Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.reset_index(drop = True)\ntrain_data = Ner_Data(df_train)\ndf_val = df_val.reset_index(drop = True)\nval_data = Ner_Data(df_val)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:16:52.588791Z","iopub.execute_input":"2023-04-27T15:16:52.589126Z","iopub.status.idle":"2023-04-27T15:17:16.166898Z","shell.execute_reply.started":"2023-04-27T15:16:52.589093Z","shell.execute_reply":"2023-04-27T15:17:16.166100Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(df_train.iloc[10]['text'])\nprint(df_train.iloc[10]['labels'])\nprint(train_data[10]['labels'])\nprint((train_data[10]['input_ids']))","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:17:16.168311Z","iopub.execute_input":"2023-04-27T15:17:16.168562Z","iopub.status.idle":"2023-04-27T15:17:16.184792Z","shell.execute_reply.started":"2023-04-27T15:17:16.168528Z","shell.execute_reply":"2023-04-27T15:17:16.183908Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Officials estimate that some 5,00,000 people living in Latin American countries such as Cuba , Argentina , Mexico , Venezuela , Chile and Uruguay are eligible for citizenship .\nO O O O O O O O O B-gpe O O O B-geo O B-geo O B-geo O B-geo O B-geo O B-geo O O O O O\ntensor([-100,   16,   16,   16,   16,   16,   16,   16,   16,   16,   16,    3,\n          16,   16,   16,    2,   16,    2,   16,    2,   16,    2,   16,    2,\n          16,    2,   16,   16,   16,   16,   16, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100])\ntensor([  101,  9018,  1116, 10301,  1115,  1199,   126,   117,  3135,   117,\n         1288,  1234,  1690,  1107,  2911,  1237,  2182,  1216,  1112,  6881,\n          117,  4904,   117,  2470,   117,  7917,   117,  6504,  1105, 11752,\n         1132,  7408,  1111,  9709,   119,   102,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0])\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data.__getitem__(10)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:17:16.185877Z","iopub.execute_input":"2023-04-27T15:17:16.186495Z","iopub.status.idle":"2023-04-27T15:17:16.201745Z","shell.execute_reply.started":"2023-04-27T15:17:16.186461Z","shell.execute_reply":"2023-04-27T15:17:16.200952Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([  101,  9018,  1116, 10301,  1115,  1199,   126,   117,  3135,   117,\n          1288,  1234,  1690,  1107,  2911,  1237,  2182,  1216,  1112,  6881,\n           117,  4904,   117,  2470,   117,  7917,   117,  6504,  1105, 11752,\n          1132,  7408,  1111,  9709,   119,   102,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0]),\n 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]),\n 'labels': tensor([-100,   16,   16,   16,   16,   16,   16,   16,   16,   16,   16,    3,\n           16,   16,   16,    2,   16,    2,   16,    2,   16,    2,   16,    2,\n           16,    2,   16,   16,   16,   16,   16, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100])}"},"metadata":{}}]},{"cell_type":"code","source":"print(train_data[100])\nprint(train_data[100]['input_ids'].detach().numpy())\nprint(tokenizer.convert_ids_to_tokens(train_data[100]['input_ids'].detach().numpy()))\nprint('#####')\nfor i in train_data[100]['labels'].detach().numpy():\n    print(i)\n    print(ids_to_label.get(i))","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:17:16.203077Z","iopub.execute_input":"2023-04-27T15:17:16.203319Z","iopub.status.idle":"2023-04-27T15:17:16.353218Z","shell.execute_reply.started":"2023-04-27T15:17:16.203287Z","shell.execute_reply":"2023-04-27T15:17:16.352552Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([  101,  1109,   158,   119,   156,   119,  7988,  1111, 11023,  1314,\n         1989,  2103,  1861,  3690,   119,   102,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([-100,   16,    5,   13,   13,   13,   16,   16,   16,   16,   16,   16,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100])}\n[  101  1109   158   119   156   119  7988  1111 11023  1314  1989  2103\n  1861  3690   119   102     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0]\n['[CLS]', 'The', 'U', '.', 'S', '.', 'Campaign', 'for', 'Burma', 'last', 'week', 'reported', 'similar', 'attacks', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n#####\n-100\nNone\n16\nO\n5\nB-org\n13\nI-org\n13\nI-org\n13\nI-org\n16\nO\n16\nO\n16\nO\n16\nO\n16\nO\n16\nO\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n-100\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_data, batch_size=16,shuffle=False)\nval_dataloader = DataLoader(val_data, batch_size=16, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:17:16.354249Z","iopub.execute_input":"2023-04-27T15:17:16.354481Z","iopub.status.idle":"2023-04-27T15:17:16.359586Z","shell.execute_reply.started":"2023-04-27T15:17:16.354445Z","shell.execute_reply":"2023-04-27T15:17:16.358857Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.0001\nbatch_size = 16\nepochs = 6","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:17:16.369134Z","iopub.execute_input":"2023-04-27T15:17:16.369444Z","iopub.status.idle":"2023-04-27T15:17:16.376329Z","shell.execute_reply.started":"2023-04-27T15:17:16.369412Z","shell.execute_reply":"2023-04-27T15:17:16.375844Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def train_loop(train_dataloader,val_dataloader,model, optimizer,epochs):\n    size = len(train_dataloader.dataset)\n    Val_loss, Train_loss = [], []\n    Train_with_accuracy, Val_with_accuracy = [], []\n    Train_without_accuracy, Val_without_accuracy = [], []\n    for epoch_num in range(epochs):\n        print(f'Epochs: {epoch_num + 1}\\n-------------------------------')\n        \n        train_loss, train_with_accuracy, train_without_accuracy = 0, 0, 0\n        train_steps = 0\n        model.train()\n        for idx,batch in enumerate(train_dataloader):\n            optimizer.zero_grad()\n            ids= batch['input_ids'].to(device)\n            mask= batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            preds = model(input_ids=ids, attention_mask=mask ,labels = labels)\n    #         print(f\"loss: {loss.item()}\")\n            loss = preds['loss']\n            logits = preds['logits']\n            train_loss+=loss.item()\n            train_steps+=1\n            # computing train accuracy\n            flattened_targets = labels.view(-1) \n            active_logits = logits.view(-1, model.num_labels) \n            flattened_predictions = torch.argmax(active_logits, axis=1) \n\n            # computing accuracy at active labels\n            labels_without = labels\n            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n\n            labels = torch.masked_select(flattened_targets, active_accuracy)\n            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n\n            tmp_train_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n            train_with_accuracy += tmp_train_accuracy\n            # computing accuracy at active labels excluding O\n            active_without_accuracy = []\n            for label in labels_without.view(-1):\n                if(label == label_to_ids['O'] or label == -100):\n                    active_without_accuracy.append(False)\n                else:\n                    active_without_accuracy.append(True)\n\n            active_without_accuracy = torch.as_tensor(active_without_accuracy)\n            active_without_accuracy = active_without_accuracy.to(device)\n\n            labels_without = torch.masked_select(flattened_targets, active_without_accuracy)\n            predictions_without = torch.masked_select(flattened_predictions, active_without_accuracy)            \n            tmp_train_without_accuracy = accuracy_score(labels_without.cpu().numpy(), predictions_without.cpu().numpy())\n            train_without_accuracy += tmp_train_without_accuracy\n\n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            if((idx+1) % 1000==0):\n                print(f\"Train Loss per 1000 steps: {train_loss/(idx+1):>4f} [{(idx+1):>5d}/{size/batch_size}]\")\n            \n        model.eval()\n        eval_loss, eval_with_accuracy, eval_without_accuracy = 0, 0, 0\n        eval_steps = 0\n\n        with torch.no_grad():\n            for idx, batch in enumerate(val_dataloader):\n                ids = batch['input_ids'].to(device, dtype = torch.long)\n                mask = batch['attention_mask'].to(device, dtype = torch.long)\n                labels = batch['labels'].to(device, dtype = torch.long)\n                preds = model(input_ids=ids, attention_mask=mask, labels=labels)\n\n                loss = preds['loss']\n                eval_logits = preds['logits'] \n    #             print(loss.item())\n                eval_loss += loss.item()\n                eval_steps += 1\n\n                if (idx+1) % 100==0:\n                    loss_step = eval_loss/eval_steps\n                    print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n\n                # computing evaluation accuracy\n                flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n                active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n                flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n\n                # only compute accuracy at active labels\n                labels_without = labels\n                active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n\n                labels = torch.masked_select(flattened_targets, active_accuracy)\n                predictions = torch.masked_select(flattened_predictions, active_accuracy)\n\n                tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n                eval_with_accuracy += tmp_eval_accuracy\n       # only compute accuracy at active labels excluding O\n                active_without_accuracy = []\n                for label in labels_without.view(-1):\n                    if(label == label_to_ids['O'] or label == -100):\n                        active_without_accuracy.append(False)\n                    else:\n                        active_without_accuracy.append(True)\n\n                active_without_accuracy = torch.as_tensor(active_without_accuracy)\n                active_without_accuracy = active_without_accuracy.to(device)\n    #             print(active_without_accuracy.size())\n    #             print(flattened_targets.size())\n\n                labels_without = torch.masked_select(flattened_targets, active_without_accuracy)\n                predictions_without = torch.masked_select(flattened_predictions, active_without_accuracy)            \n                tmp_eval_without_accuracy = accuracy_score(labels_without.cpu().numpy(), predictions_without.cpu().numpy())\n                eval_without_accuracy += tmp_eval_without_accuracy\n\n        train_loss = train_loss / train_steps\n        train_with_accuracy = train_with_accuracy / train_steps\n        train_without_accuracy = train_without_accuracy / train_steps\n        Train_loss.append(train_loss)\n        Train_with_accuracy.append(train_with_accuracy)\n        Train_without_accuracy.append(train_without_accuracy)\n        print(f\"Total Train Loss: {train_loss}\")\n        print(f\"Total Train Accuracy With O: {train_with_accuracy}\")\n        print(f\"Total Train Accuracy Without O: {train_without_accuracy}\")\n        \n        eval_loss = eval_loss / eval_steps\n        eval_with_accuracy = eval_with_accuracy / eval_steps\n        eval_without_accuracy = eval_without_accuracy / eval_steps\n        Val_loss.append(eval_loss)\n        Val_with_accuracy.append(eval_with_accuracy)\n        Val_without_accuracy.append(eval_without_accuracy)\n        print(f\"Total Validation Loss: {eval_loss}\")\n        print(f\"Total Validation Accuracy With O: {eval_with_accuracy}\")\n        print(f\"Total Validation Accuracy Without O: {eval_without_accuracy}\")\n    \n    item = {}\n    item['train_loss'] = Train_loss\n    item['val_loss'] = Val_loss\n    item['train_with_acc'] = Train_with_accuracy\n    item['val_with_acc'] = Val_with_accuracy\n    item['train_without_acc'] = Train_without_accuracy\n    item['val_without_acc'] = Val_without_accuracy\n    return item\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:17:16.396368Z","iopub.execute_input":"2023-04-27T15:17:16.396920Z","iopub.status.idle":"2023-04-27T15:17:16.418996Z","shell.execute_reply.started":"2023-04-27T15:17:16.396833Z","shell.execute_reply":"2023-04-27T15:17:16.418221Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(label_to_ids))\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:17:16.420765Z","iopub.execute_input":"2023-04-27T15:17:16.421230Z","iopub.status.idle":"2023-04-27T15:17:23.108526Z","shell.execute_reply.started":"2023-04-27T15:17:16.421196Z","shell.execute_reply":"2023-04-27T15:17:23.107708Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=17, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nresults = train_loop(train_dataloader,val_dataloader, model, optimizer,epochs)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:17:23.109889Z","iopub.execute_input":"2023-04-27T15:17:23.110169Z","iopub.status.idle":"2023-04-27T20:30:05.915161Z","shell.execute_reply.started":"2023-04-27T15:17:23.110134Z","shell.execute_reply":"2023-04-27T20:30:05.913850Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Epochs: 1\n-------------------------------\nTrain Loss per 1000 steps: 0.269061 [ 1000/2397.9375]\nTrain Loss per 1000 steps: 0.226578 [ 2000/2397.9375]\nValidation loss per 100 evaluation steps: 0.16732494205236434\nValidation loss per 100 evaluation steps: 0.16624102910980582\nValidation loss per 100 evaluation steps: 0.16483251477281252\nTotal Train Loss: 0.21705353971232066\nTotal Train Accuracy With O: 0.9336092961694766\nTotal Train Accuracy Without O: 0.6828501795677945\nTotal Validation Loss: 0.16483251477281252\nTotal Validation Accuracy With O: 0.9475841599314698\nTotal Validation Accuracy Without O: 0.7438464128409162\nEpochs: 2\n-------------------------------\nTrain Loss per 1000 steps: 0.149881 [ 1000/2397.9375]\nTrain Loss per 1000 steps: 0.142647 [ 2000/2397.9375]\nValidation loss per 100 evaluation steps: 0.16365802075713873\nValidation loss per 100 evaluation steps: 0.1625620689522475\nValidation loss per 100 evaluation steps: 0.1614482051320374\nTotal Train Loss: 0.14045146165619152\nTotal Train Accuracy With O: 0.9552212412516871\nTotal Train Accuracy Without O: 0.7872091709823381\nTotal Validation Loss: 0.1614482051320374\nTotal Validation Accuracy With O: 0.9515258573684116\nTotal Validation Accuracy Without O: 0.7577831773252585\nEpochs: 3\n-------------------------------\nTrain Loss per 1000 steps: 0.122629 [ 1000/2397.9375]\nTrain Loss per 1000 steps: 0.118516 [ 2000/2397.9375]\nValidation loss per 100 evaluation steps: 0.1563493838906288\nValidation loss per 100 evaluation steps: 0.15444016521796583\nValidation loss per 100 evaluation steps: 0.15345680365959805\nTotal Train Loss: 0.11793271516153808\nTotal Train Accuracy With O: 0.9618660783974055\nTotal Train Accuracy Without O: 0.8209319563327583\nTotal Validation Loss: 0.15345680365959805\nTotal Validation Accuracy With O: 0.9549881926514757\nTotal Validation Accuracy Without O: 0.7874411410900947\nEpochs: 4\n-------------------------------\nTrain Loss per 1000 steps: 0.102306 [ 1000/2397.9375]\nTrain Loss per 1000 steps: 0.099592 [ 2000/2397.9375]\nValidation loss per 100 evaluation steps: 0.16457560516893863\nValidation loss per 100 evaluation steps: 0.16156427984125912\nValidation loss per 100 evaluation steps: 0.15953611786787708\nTotal Train Loss: 0.09834922020136377\nTotal Train Accuracy With O: 0.9675153292424106\nTotal Train Accuracy Without O: 0.8482824552630105\nTotal Validation Loss: 0.15953611786787708\nTotal Validation Accuracy With O: 0.9548855857711934\nTotal Validation Accuracy Without O: 0.7846700839021589\nEpochs: 5\n-------------------------------\nTrain Loss per 1000 steps: 0.088017 [ 1000/2397.9375]\nTrain Loss per 1000 steps: 0.085172 [ 2000/2397.9375]\nValidation loss per 100 evaluation steps: 0.16095480801537632\nTrain Loss per 1000 steps: 0.075664 [ 1000/2397.9375]\nTrain Loss per 1000 steps: 0.076141 [ 2000/2397.9375]\nValidation loss per 100 evaluation steps: 0.17052491534501313\nValidation loss per 100 evaluation steps: 0.16843145601451398\nValidation loss per 100 evaluation steps: 0.167963203197966\nTotal Train Loss: 0.07535196172020664\nTotal Train Accuracy With O: 0.9752086399142701\nTotal Train Accuracy Without O: 0.8859644854164269\nTotal Validation Loss: 0.167963203197966\nTotal Validation Accuracy With O: 0.9587978988103955\nTotal Validation Accuracy Without O: 0.8127963960468906\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nEPOCH = np.arange(1,7)\nplt.plot(EPOCH,results['train_loss'], label = 'train_loss')\n\nplt.plot(EPOCH,results['val_loss'], label = 'val_loss')\nplt.xlabel(\"EPOCHS\")\nplt.ylabel(\"Loss\")\nplt.title('Training v/s Validation Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:26:22.874115Z","iopub.execute_input":"2023-04-27T21:26:22.874386Z","iopub.status.idle":"2023-04-27T21:26:23.089343Z","shell.execute_reply.started":"2023-04-27T21:26:22.874355Z","shell.execute_reply":"2023-04-27T21:26:23.088639Z"},"trusted":true},"execution_count":83,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"test_dataset = df_test.reset_index(drop = True)\ntest_data = Ner_Data(test_dataset)\ntest_data_loader = DataLoader(test_data, batch_size=4, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T22:29:18.818058Z","iopub.execute_input":"2023-04-27T22:29:18.818341Z","iopub.status.idle":"2023-04-27T22:29:21.222299Z","shell.execute_reply.started":"2023-04-27T22:29:18.818311Z","shell.execute_reply":"2023-04-27T22:29:21.221541Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"def test(model, testing_loader):\n    test_loss, test_with_accuracy, test_without_accuracy = 0, 0, 0\n    test_steps = 0\n    test_preds, test_labels = [], []\n    \n    for idx, batch in enumerate(testing_loader):\n\n        ids = batch['input_ids'].to(device, dtype = torch.long)\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\n        labels = batch['labels'].to(device, dtype = torch.long)\n        preds= model(input_ids=ids, attention_mask=mask, labels=labels)\n\n        loss = preds['loss']\n        logits = preds['logits'] \n\n        test_loss += loss.item()\n        test_steps += 1\n        # compute evaluation accuracy\n        flattened_targets = labels.view(-1) \n        active_logits = logits.view(-1, model.num_labels) \n        flattened_predictions = torch.argmax(active_logits, axis=1)\n\n        # only compute accuracy at active labels\n        labels_without = labels\n        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n\n        labels = torch.masked_select(flattened_targets, active_accuracy)\n        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n\n        test_labels.extend(labels)\n        test_preds.extend(predictions)\n\n        tmp_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n        test_with_accuracy += tmp_accuracy\n\n        active_without_accuracy = []\n        for label in labels_without.view(-1):\n            if(label == label_to_ids['O'] or label == -100):\n                active_without_accuracy.append(False)\n            else:\n                active_without_accuracy.append(True)\n\n        active_without_accuracy = torch.as_tensor(active_without_accuracy)\n        active_without_accuracy = active_without_accuracy.to(device)\n#             print(active_without_accuracy.size())\n#             print(flattened_targets.size())\n\n        labels_without = torch.masked_select(flattened_targets, active_without_accuracy)\n        predictions_without = torch.masked_select(flattened_predictions, active_without_accuracy)            \n        tmp_without_accuracy = accuracy_score(labels_without.cpu().numpy(), predictions_without.cpu().numpy())\n        test_without_accuracy += tmp_without_accuracy\n            \n    labels = [ids_to_label[id.item()] for id in test_labels]\n    predictions = [ids_to_label[id.item()] for id in test_preds]\n    test_loss = test_loss / test_steps\n    test_with_accuracy = test_with_accuracy / test_steps\n    test_without_accuracy = test_without_accuracy / test_steps\n    print(f\"Test Loss: {test_loss}\")\n    print(f\"Test Accuracy: {test_with_accuracy}\")\n    print(f\"Test Accuracy Without O: {test_without_accuracy}\")\n    return labels, predictions","metadata":{"execution":{"iopub.status.busy":"2023-04-27T22:29:22.966407Z","iopub.execute_input":"2023-04-27T22:29:22.966663Z","iopub.status.idle":"2023-04-27T22:29:22.999104Z","shell.execute_reply.started":"2023-04-27T22:29:22.966633Z","shell.execute_reply":"2023-04-27T22:29:22.998295Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"labels, predictions = test(model, test_data_loader)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T22:29:24.070345Z","iopub.execute_input":"2023-04-27T22:29:24.070600Z","iopub.status.idle":"2023-04-27T22:32:50.977245Z","shell.execute_reply.started":"2023-04-27T22:29:24.070571Z","shell.execute_reply":"2023-04-27T22:32:50.976384Z"},"trusted":true},"execution_count":130,"outputs":[{"name":"stdout","text":"Test Loss: 0.1638020795021373\nTest Accuracy: 0.9603827639657017\nTest Accuracy Without O: 0.815912740379854\n","output_type":"stream"}]},{"cell_type":"code","source":"from seqeval.metrics import classification_report\nprint(classification_report([labels], [predictions]))","metadata":{"execution":{"iopub.status.busy":"2023-04-27T22:33:34.444314Z","iopub.execute_input":"2023-04-27T22:33:34.444588Z","iopub.status.idle":"2023-04-27T22:33:36.393389Z","shell.execute_reply.started":"2023-04-27T22:33:34.444556Z","shell.execute_reply":"2023-04-27T22:33:36.392589Z"},"trusted":true},"execution_count":131,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         art       0.30      0.13      0.18        55\n         eve       0.17      0.12      0.14        17\n         geo       0.80      0.87      0.83      4625\n         gpe       0.96      0.89      0.93      1794\n         nat       1.00      0.10      0.18        50\n         org       0.71      0.65      0.68      2574\n         per       0.73      0.76      0.74      2271\n         tim       0.83      0.83      0.83      2115\n\n   micro avg       0.79      0.80      0.80     13501\n   macro avg       0.69      0.54      0.56     13501\nweighted avg       0.79      0.80      0.79     13501\n\n","output_type":"stream"}]},{"cell_type":"code","source":"def align_word_ids(texts):\n  \n    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n    word_ids = tokenized_inputs.word_ids()\n    \n    previous_word_idx = None\n    label_ids = []\n\n    for word_idx in word_ids:\n\n        if word_idx is None:\n            label_ids.append(-100)\n\n        elif word_idx != previous_word_idx:\n            try:\n                label_ids.append(1)\n            except:\n                label_ids.append(-100)\n        else:\n            try:\n                label_ids.append(1 if label_all_tokens else -100)\n            except:\n                label_ids.append(-100)\n        previous_word_idx = word_idx\n\n    return label_ids\n\n\ndef evaluate_one_text(model, sentence):\n    text_input = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True)\n    text = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n    mask = text['attention_mask'].to(device)\n    input_id = text['input_ids'].to(device)\n    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n\n    logits = model(input_id, mask, None)\n    logits_clean = logits[0][label_ids != -100]\n\n    predictions = logits_clean.argmax(dim=1).tolist()\n    prediction_label = [ids_to_label[i] for i in predictions]\n    text_tokens = tokenizer.convert_ids_to_tokens(text_input['input_ids'])\n    \n    print(sentence)\n    print(\"Tokenized Sentence:\",text_tokens[1:text_tokens.index('[SEP]')])\n    print(\"Predicted Labels:\",prediction_label)\n            \nevaluate_one_text(model, 'Sundar Pichai is the CEO of Google .')\n","metadata":{"execution":{"iopub.status.busy":"2023-04-27T22:10:10.946427Z","iopub.execute_input":"2023-04-27T22:10:10.946706Z","iopub.status.idle":"2023-04-27T22:10:10.984933Z","shell.execute_reply.started":"2023-04-27T22:10:10.946676Z","shell.execute_reply":"2023-04-27T22:10:10.984236Z"},"trusted":true},"execution_count":108,"outputs":[{"name":"stdout","text":"Sundar Pichai is the CEO of Google .\nTokenized Sentence: ['Sun', '##dar', 'Pi', '##cha', '##i', 'is', 'the', 'CEO', 'of', 'Google', '.']\nPredicted Labels: ['B-per', 'B-per', 'I-per', 'I-per', 'I-per', 'O', 'O', 'O', 'O', 'B-org', 'O']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## SAVING THE MODEL","metadata":{}},{"cell_type":"code","source":"torch.save(model, './model.pth')","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:30:06.081785Z","iopub.execute_input":"2023-04-27T20:30:06.082184Z","iopub.status.idle":"2023-04-27T20:30:06.089103Z","shell.execute_reply.started":"2023-04-27T20:30:06.082140Z","shell.execute_reply":"2023-04-27T20:30:06.088461Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# model3 = torch.load('./model.pth')","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:30:06.090060Z","iopub.execute_input":"2023-04-27T20:30:06.090469Z","iopub.status.idle":"2023-04-27T20:30:06.097354Z","shell.execute_reply.started":"2023-04-27T20:30:06.090436Z","shell.execute_reply":"2023-04-27T20:30:06.096502Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}